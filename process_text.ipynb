{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import stem\n",
    "from nltk.corpus import stopwords\n",
    "import unicodedata\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, RegexpParser\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from nltk.util import ngrams\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_text_with_features(model,input_text):\n",
    "\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('omw-1.4')\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "\n",
    "    df= pd.DataFrame({'text': [input_text]})\n",
    "\n",
    "\n",
    "    # remove special characters\n",
    "    df['text'] = df['text'].str.replace(r'[^A-Za-z0-9\\s]', '', regex=True)\n",
    "\n",
    "    # Lowercase the text\n",
    "    df['text'] = df['text'].str.lower()\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    df['text'] = df['text'].str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "\n",
    "    \n",
    "    # Tokenize the text\n",
    "    df['tokens'] = df['text'].str.split()\n",
    "    \n",
    "    # Remove stop words\n",
    "    df['tokens'] = df['text'].apply(word_tokenize)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    df['tokens'] = df['tokens'].apply(lambda tokens: [word for word in tokens if word.lower() not in stop_words])\n",
    "    \n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    df['lemmas'] = df['tokens'].apply(\n",
    "    lambda tokens: [lemmatizer.lemmatize(token) for token in tokens])\n",
    "\n",
    "    # text length and word count\n",
    "    df['text_length'] = df['text'].str.len()\n",
    "    df['word_count'] = df['text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "\n",
    "    content_tags = {'NN', 'VB', 'JJ', 'RB'}\n",
    "\n",
    "    def calculate_lexical_density(lemmas):\n",
    "        # POS tagging\n",
    "        pos_tags = pos_tag(lemmas)\n",
    "        \n",
    "        # Filter lexical words\n",
    "        lexical_words = [word for word, tag in pos_tags if tag[:2] in content_tags]\n",
    "        \n",
    "        # Calculate lexical density\n",
    "        if len(lemmas) > 0:\n",
    "            return len(lexical_words) / len(lemmas)\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    df['lexical_density'] = df['lemmas'].apply(calculate_lexical_density)\n",
    "\n",
    "\n",
    "    def calculate_grammatical_complexity(text):\n",
    "        # Tokenize the text into sentences\n",
    "        sentences = sent_tokenize(text)\n",
    "        \n",
    "        # Initialize variables for counting\n",
    "        total_words = 0\n",
    "        total_clauses = 0\n",
    "        total_sentences = len(sentences)\n",
    "        \n",
    "        # For each sentence, tokenize words and count clauses\n",
    "        for sentence in sentences:\n",
    "            words = word_tokenize(sentence)\n",
    "            total_words += len(words)\n",
    "            \n",
    "            # Count clauses based on the occurrence of coordinating conjunctions (e.g., \"and\", \"but\")\n",
    "            # You can expand this list as needed\n",
    "            conjunctions = ['and', 'but', 'or', 'nor', 'for', 'so', 'yet']\n",
    "            clauses = sum([1 for word in words if word.lower() in conjunctions])\n",
    "            \n",
    "            total_clauses += clauses + 1  # Each sentence is considered at least 1 clause\n",
    "        \n",
    "        # Calculate average sentence length (in words)\n",
    "        avg_sentence_length = total_words / total_sentences if total_sentences > 0 else 0\n",
    "        \n",
    "        # Calculate the average number of clauses per sentence\n",
    "        avg_clauses_per_sentence = total_clauses / total_sentences if total_sentences > 0 else 0\n",
    "        \n",
    "        # Combine metrics into a \"grammatical complexity\" score\n",
    "        grammatical_complexity_score = avg_sentence_length * avg_clauses_per_sentence\n",
    "        \n",
    "        return grammatical_complexity_score\n",
    "\n",
    "    df['grammatical_complexity'] = df['text'].apply(calculate_grammatical_complexity)\n",
    "\n",
    "    # Function to calculate syntactic complexity with coordination, relative clauses, and subordination\n",
    "    def calculate_syntactic_complexity(text):\n",
    "        # Tokenize the text into sentences\n",
    "        sentences = sent_tokenize(text)\n",
    "        \n",
    "        # Initialize variables for counting\n",
    "        total_tokens = 0\n",
    "        total_phrases = 0  # Count of noun phrases, verb phrases, adjective phrases, etc.\n",
    "        total_coordination = 0\n",
    "        total_subordination = 0\n",
    "        total_relative_clauses = 0\n",
    "        total_sentences = len(sentences)\n",
    "        \n",
    "        # For each sentence, tokenize and perform POS tagging\n",
    "        for sentence in sentences:\n",
    "            words = word_tokenize(sentence)\n",
    "            pos_tags = pos_tag(words)\n",
    "            total_tokens += len(words)\n",
    "            \n",
    "            # Define a more comprehensive grammar to include noun phrases, verb phrases,\n",
    "            # adjective phrases, prepositional phrases, coordination, relative clauses, and subordination\n",
    "            grammar = r\"\"\"\n",
    "                NP: {<DT>?<JJ>*<NN>}         # Noun Phrase\n",
    "                VP: {<VB.*><NP|PP>*}          # Verb Phrase\n",
    "                AP: {<JJ>}                    # Adjective Phrase\n",
    "                PP: {<IN><NP>}                # Prepositional Phrase\n",
    "                CC: {<CC>}                    # Coordinating Conjunction\n",
    "                RC: {<WP|WDT><VB.*><NP>}      # Relative Clause (e.g., who, that, which)\n",
    "                SBAR: {<IN><S>}               # Subordinating Clause (e.g., because, if)\n",
    "            \"\"\"\n",
    "            \n",
    "            # Create a parser based on the defined grammar\n",
    "            cp = RegexpParser(grammar)\n",
    "            tree = cp.parse(pos_tags)\n",
    "            \n",
    "            # Count the number of noun phrases, verb phrases, adjective phrases, prepositional phrases\n",
    "            # as well as coordination, relative clauses, and subordination\n",
    "            total_phrases += sum(1 for subtree in tree.subtrees() if subtree.label() in ['NP', 'VP', 'AP', 'PP'])\n",
    "\n",
    "        \n",
    "        # Calculate the average number of phrases per sentence\n",
    "        avg_phrases_per_sentence = total_phrases / total_sentences if total_sentences > 0 else 0\n",
    "        syntactic_complexity_score = total_tokens / avg_phrases_per_sentence if avg_phrases_per_sentence > 0 else 0\n",
    "        \n",
    "        return syntactic_complexity_score\n",
    "    \n",
    "    df['syntactic_complexity'] = df['text'].apply(calculate_syntactic_complexity).apply(pd.Series)\n",
    "\n",
    "\n",
    "        # Define specific words related to AI-generated text\n",
    "    specific_words = {\n",
    "        'hedging': [\"maybe\", \"perhaps\", \"possibly\", \"likely\", \"potentially\", \"could\", \"would\", \"might\", \"seem\", \"suggest\", \"appears\", \"likely\", \"suggests\"],\n",
    "        'filler': [\"like\", \"you know\", \"actually\", \"basically\", \"seriously\", \"literally\", \"i mean\", \"so\", \"well\"],\n",
    "        'polite': [\"could you\", \"please\", \"thank you\", \"sorry\", \"respectfully\", \"kindly\"],\n",
    "        'adverbs': [\"definitely\", \"absolutely\", \"certainly\", \"undoubtedly\", \"clearly\", \"extremely\", \"completely\", \"fully\"],\n",
    "        'quantifiers': [\"all\", \"every\", \"some\", \"many\", \"few\", \"most\"],\n",
    "        'ai_terms': [\"algorithm\", \"model\", \"data\", \"training\", \"machine learning\", \"artificial intelligence\", \"neural network\", \"deep learning\", \"automation\"]\n",
    "    }\n",
    "\n",
    "    # Define negation words\n",
    "    negation_words = [\"not\", \"no\", \"never\", \"none\", \"nothing\", \"nobody\", \"neither\", \"nowhere\", \"doesn't\", \"isn't\", \"wasn't\", \"aren't\", \"can't\", \"won't\", \"shouldn't\", \"haven't\", \"didn't\"]\n",
    "\n",
    "    # Define words for vague language\n",
    "    vague_words = [\"things\", \"stuff\", \"aspects\", \"elements\", \"factors\", \"issues\"]\n",
    "\n",
    "    # Define passive voice expressions (simplified for demonstration)\n",
    "    passive_phrases = [\"is done\", \"has been performed\", \"is considered\"]\n",
    "\n",
    "    # Function to count specific words and patterns\n",
    "    def analyze_lemmas_for_specific_characteristics(lemmas):\n",
    "        text = \" \".join(lemmas).lower()  # Convert lemmas to lowercase string for easier matching\n",
    "        \n",
    "        # Count specific words related to AI-generated text\n",
    "        hedging_count = sum(1 for word in lemmas if word in specific_words['hedging'])\n",
    "        filler_count = sum(1 for word in lemmas if word in specific_words['filler'])\n",
    "        polite_count = sum(1 for word in lemmas if word in specific_words['polite'])\n",
    "        adverb_count = sum(1 for word in lemmas if word in specific_words['adverbs'])\n",
    "        quantifier_count = sum(1 for word in lemmas if word in specific_words['quantifiers'])\n",
    "        ai_terms_count = sum(1 for word in lemmas if word in specific_words['ai_terms'])\n",
    "        \n",
    "        # Count negation words\n",
    "        negation_count = sum(1 for word in lemmas if word in negation_words)\n",
    "        \n",
    "        # Count vague words\n",
    "        vague_word_count = sum(1 for word in lemmas if word in vague_words)\n",
    "        \n",
    "        # Count passive voice phrases (simplified check for combinations of words in passive_phrases)\n",
    "        passive_count = sum(1 for phrase in passive_phrases if phrase in text)\n",
    "        \n",
    "        return {\n",
    "            'hedging_word_count': hedging_count,\n",
    "            'filler_word_count': filler_count,\n",
    "            'polite_word_count': polite_count,\n",
    "            'adverb_word_count': adverb_count,\n",
    "            'quantifier_word_count': quantifier_count,\n",
    "            'ai_term_count': ai_terms_count,\n",
    "            'negation_word_count': negation_count,\n",
    "            'vague_word_count': vague_word_count,\n",
    "            'passive_voice_count': passive_count\n",
    "        }\n",
    "    \n",
    "        # Example of applying this function to the 'lemmas' column\n",
    "    df_features_lemmas = df['lemmas'].apply(analyze_lemmas_for_specific_characteristics)\n",
    "\n",
    "    # Normalize the result and merge with the original dataframe\n",
    "    df_features_lemmas = pd.json_normalize(df_features_lemmas)\n",
    "\n",
    "    # Add the features back to the original dataframe\n",
    "    df = pd.concat([df, df_features_lemmas], axis=1)\n",
    "\n",
    "    # Function to calculate textual entropy\n",
    "    def calculate_entropy(tokens):\n",
    "        # Count the frequency of each token\n",
    "        token_counts = Counter(tokens)\n",
    "        \n",
    "        # Total number of tokens\n",
    "        total_tokens = len(tokens)\n",
    "        \n",
    "        if total_tokens == 0:\n",
    "            return 0  # Return 0 if the list is empty\n",
    "        \n",
    "        # Calculate probabilities and entropy\n",
    "        probabilities = [count / total_tokens for count in token_counts.values()]\n",
    "        entropy = -sum(p * np.log2(p) for p in probabilities if p > 0)  # Avoid log(0)\n",
    "        \n",
    "        return entropy\n",
    "    \n",
    "    df['textual_entropy'] = df['tokens'].apply(calculate_entropy)\n",
    "\n",
    "    df['is_ai'] = 0\n",
    "\n",
    "\n",
    "    # Load the saved vectorizer\n",
    "    with open('tfidf_vectorizer.pkl', 'rb') as file:\n",
    "        vectorizer = pickle.load(file)\n",
    "\n",
    "    # Separate the target variable and features\n",
    "    X = df.drop(columns=['is_ai'])\n",
    "    y = df['is_ai']\n",
    "\n",
    "    # Drop the 'tokens' and 'lemmas' columns as you don't want them\n",
    "    X = X.drop(columns=['tokens', 'lemmas'])\n",
    "\n",
    "    # Extract text column and other numerical columns\n",
    "    X_text = X['text']  # Text column\n",
    "    X_features = X.drop(columns=['text'])  # Other features\n",
    "\n",
    "    # Transform the text data using the loaded vectorizer\n",
    "    X_text_vec = vectorizer.transform(X_text)\n",
    "\n",
    "    # Handle non-numeric columns in X_features\n",
    "    X_numeric_features = X_features.select_dtypes(include=[np.number])\n",
    "\n",
    "    # Scale the numeric features\n",
    "    scaler = StandardScaler()\n",
    "    X_numeric_scaled = scaler.fit_transform(X_numeric_features)\n",
    "\n",
    "    # Combine the vectorized text features and scaled numeric features\n",
    "    X_combined = hstack((X_text_vec, X_numeric_scaled))\n",
    "\n",
    "    # Create a new DataFrame with combined features and target variable\n",
    "    df_processed = pd.DataFrame.sparse.from_spmatrix(X_combined)\n",
    "\n",
    "    # Add back the non-text columns (keeping the ones you want)\n",
    "    non_numeric_columns = X_features.select_dtypes(exclude=[np.number]).columns\n",
    "    df_processed[non_numeric_columns] = X_features[non_numeric_columns]\n",
    "\n",
    "    # Add the target variable back to the DataFrame\n",
    "    df_processed['is_ai'] = y\n",
    "\n",
    "\n",
    "    # drop is_ai column\n",
    "    df_processed = df_processed.drop(columns=['is_ai'])\n",
    "\n",
    "    prediction = model.predict(df_processed)\n",
    "\n",
    "    if prediction == 1:\n",
    "        print(\"The text is AI-generated.\")\n",
    "    else:\n",
    "        print(\"The text is human-generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rf_model_1.pkl', 'rb') as file:\n",
    "    model_rf = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vehnie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vehnie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vehnie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\vehnie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\vehnie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text is AI-generated.\n"
     ]
    }
   ],
   "source": [
    "df= format_text_with_features( model_rf,\n",
    "\"\"\"\"The Clockmaker's Secret\n",
    "\n",
    "In the heart of a bustling city, hidden within a narrow alley, stood a tiny shop with a faded sign that read “Gideon's Timepieces.” The shop's windows were cluttered with an assortment of antique clocks, each ticking in a peculiar harmony that seemed to slow time itself. Gideon, the enigmatic clockmaker, was known for his unparalleled craftsmanship—and his uncanny ability to fix any broken clock brought to him.\n",
    "\n",
    "Few knew, however, that Gideon's skill came from a secret he guarded fiercely. Beneath the shop, in a room lit only by flickering candlelight, was a clock unlike any other. It was massive, its gears larger than wagon wheels and its pendulum glowing faintly with an otherworldly blue hue. Gideon called it the Eternal Mechanism.\n",
    "\n",
    "This clock didn’t just measure time; it controlled it. With a twist of a golden key, Gideon could turn back hours, leap forward days, or freeze a moment entirely. For years, he had used the Mechanism sparingly—just enough to fix mistakes or grant himself a second chance. But the power came with rules: never change the past too drastically, and never, ever let anyone else know.\n",
    "\n",
    "One rainy evening, a young woman named Clara stumbled into the shop. Her eyes were red from crying, and in her hands was a pocket watch shattered beyond recognition.\n",
    "\n",
    "“It was my father’s,” she whispered. “He passed away last week, and this is all I have left of him. Can you fix it?”\n",
    "\n",
    "Gideon examined the watch. Its face was cracked, its hands bent, and its mechanism utterly destroyed. Yet, he saw something else—something he hadn’t seen in years. The faint glow of the Eternal Mechanism shimmered in the broken gears, as if the pocket watch had once been tied to its power.\n",
    "\n",
    "“Leave it with me,” he said, his voice soft.\n",
    "\n",
    "For days, Gideon worked on the watch, piecing together not just its mechanism but its connection to the greater clock below. He realized that it wasn’t just a keepsake—it was a key, like the golden one he carried. Clara’s father must have been a Keeper of Time, a role passed through generations, though she seemed unaware of her inheritance.\n",
    "\n",
    "When the watch was finally restored, Gideon hesitated. Should he tell Clara the truth? That she held a piece of the Eternal Mechanism’s power in her hands? Or should he hide it, protecting her from the burden he had carried for so long?\n",
    "\n",
    "In the end, he simply handed her the watch and said, “Take care of this. It’s more precious than you realize.”\n",
    "\n",
    "Clara thanked him and left, but as she stepped out into the rainy night, she felt a warmth emanating from the watch—a warmth that whispered of second chances and endless possibilities.\n",
    "\n",
    "And in his workshop, Gideon smiled faintly, knowing that time’s secrets were safe… for now.\n",
    "\n",
    "Let me know if you'd like to expand or adjust this story!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [0]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Prediction: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

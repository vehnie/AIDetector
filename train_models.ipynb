{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vehnie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import stem\n",
    "from nltk.corpus import stopwords\n",
    "import unicodedata\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, RegexpParser\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from nltk.util import ngrams\n",
    "import nltk\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "def preprocess_data(df):\n",
    "    # Separate the target variable and features\n",
    "    X = df.drop(columns=['is_ai'])\n",
    "    y = df['is_ai']\n",
    "    \n",
    "    # Drop the 'tokens' and 'lemmas' columns as you don't want them\n",
    "    X = X.drop(columns=['tokens', 'lemmas'])\n",
    "    \n",
    "    # Extract text column and other numerical columns\n",
    "    X_text = X['text']  # Text column\n",
    "    X_features = X.drop(columns=['text'])  # Other features\n",
    "\n",
    "    # Vectorize the text data using TF-IDF\n",
    "    vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    X_text_vec = vectorizer.fit_transform(X_text)\n",
    "\n",
    "    # Handle non-numeric columns in X_features\n",
    "    X_numeric_features = X_features.select_dtypes(include=[np.number])\n",
    "\n",
    "    # Scale the numeric features\n",
    "    scaler = StandardScaler()\n",
    "    X_numeric_scaled = scaler.fit_transform(X_numeric_features)\n",
    "\n",
    "    # Combine the vectorized text features and scaled numeric features\n",
    "    X_combined = hstack((X_text_vec, X_numeric_scaled))\n",
    "\n",
    "    # Create a new DataFrame with combined features and target variable\n",
    "    df_processed = pd.DataFrame.sparse.from_spmatrix(X_combined)\n",
    "\n",
    "    # Add back the non-text columns (keeping the ones you want)\n",
    "    non_numeric_columns = X_features.select_dtypes(exclude=[np.number]).columns\n",
    "    df_processed[non_numeric_columns] = X_features[non_numeric_columns]\n",
    "\n",
    "    # Add the target variable back to the DataFrame\n",
    "    df_processed['is_ai'] = y\n",
    "\n",
    "    with open('tfidf_vectorizer.pkl', 'wb') as file:\n",
    "        pickle.dump(vectorizer, file)\n",
    "        \n",
    "    return df_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed = preprocess_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>5006</th>\n",
       "      <th>5007</th>\n",
       "      <th>5008</th>\n",
       "      <th>5009</th>\n",
       "      <th>5010</th>\n",
       "      <th>5011</th>\n",
       "      <th>5012</th>\n",
       "      <th>5013</th>\n",
       "      <th>5014</th>\n",
       "      <th>is_ai</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.410252</td>\n",
       "      <td>-0.14851</td>\n",
       "      <td>-0.393955</td>\n",
       "      <td>-0.00391</td>\n",
       "      <td>-0.228649</td>\n",
       "      <td>1.824225</td>\n",
       "      <td>6.387063</td>\n",
       "      <td>-0.040032</td>\n",
       "      <td>0.708213</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 5016 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3  4  5  6  7  8  9  ...      5006     5007      5008     5009  \\\n",
       "0  0  0  0  0  0  0  0  0  0  0  ...  2.410252 -0.14851 -0.393955 -0.00391   \n",
       "\n",
       "       5010      5011      5012      5013      5014  is_ai  \n",
       "0 -0.228649  1.824225  6.387063 -0.040032  0.708213      0  \n",
       "\n",
       "[1 rows x 5016 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_processed.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.91       987\n",
      "           1       0.91      0.92      0.92      1013\n",
      "\n",
      "    accuracy                           0.92      2000\n",
      "   macro avg       0.92      0.92      0.92      2000\n",
      "weighted avg       0.92      0.92      0.92      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Assuming df_processed is your processed DataFrame\n",
    "X = df_processed.drop(columns=['is_ai'])  # Features\n",
    "y = df_processed['is_ai']  # Target variable\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the LinearSVC model (which can handle sparse matrices)\n",
    "svm_model = LinearSVC()\n",
    "\n",
    "# Train the model using the sparse matrix\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model performance\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('svm_model_1.pkl', 'wb') as f:\n",
    "    pickle.dump(svm_model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **RF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.93\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Assuming df_processed is the output of your preprocess_data function\n",
    "# df_processed = preprocess_data(df_balanced)\n",
    "\n",
    "# Separate the target variable and features\n",
    "X = df_processed.drop(columns=['is_ai'])\n",
    "y = df_processed['is_ai']\n",
    "\n",
    "# Extract the text features (sparse matrix)\n",
    "X_text_vec = X.iloc[:, :-1].sparse.to_coo()  # Exclude the target column 'is_ai' and keep sparse format\n",
    "\n",
    "# Extract numerical features (make sure they are sparse as well)\n",
    "X_numeric_scaled = X.iloc[:, -1:].sparse.to_coo()  # Assuming last column is the numerical features\n",
    "\n",
    "# Combine the sparse matrix for text features and numerical features\n",
    "X_combined = hstack((X_text_vec, X_numeric_scaled))  # Keep sparse format\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the Random Forest model\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rf_model_1.pkl', 'wb') as f:\n",
    "    pickle.dump(rf_model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Convultional Neural Network (CNN)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vehnie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 549ms/step - accuracy: 0.7838 - loss: 0.4104 - val_accuracy: 0.8930 - val_loss: 0.2341\n",
      "Epoch 2/10\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 491ms/step - accuracy: 0.9332 - loss: 0.1534 - val_accuracy: 0.9060 - val_loss: 0.2161\n",
      "Epoch 3/10\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 446ms/step - accuracy: 0.9728 - loss: 0.0716 - val_accuracy: 0.9030 - val_loss: 0.2627\n",
      "Epoch 4/10\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 445ms/step - accuracy: 0.9912 - loss: 0.0280 - val_accuracy: 0.9045 - val_loss: 0.3410\n",
      "Epoch 5/10\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 445ms/step - accuracy: 0.9958 - loss: 0.0142 - val_accuracy: 0.8865 - val_loss: 0.4735\n",
      "Epoch 6/10\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 445ms/step - accuracy: 0.9962 - loss: 0.0130 - val_accuracy: 0.8975 - val_loss: 0.4983\n",
      "Epoch 7/10\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 446ms/step - accuracy: 0.9981 - loss: 0.0125 - val_accuracy: 0.9050 - val_loss: 0.4308\n",
      "Epoch 8/10\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 447ms/step - accuracy: 0.9987 - loss: 0.0046 - val_accuracy: 0.9075 - val_loss: 0.4736\n",
      "Epoch 9/10\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 447ms/step - accuracy: 1.0000 - loss: 8.5566e-04 - val_accuracy: 0.8990 - val_loss: 0.5711\n",
      "Epoch 10/10\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 447ms/step - accuracy: 1.0000 - loss: 2.0890e-04 - val_accuracy: 0.9090 - val_loss: 0.5668\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 76ms/step - accuracy: 0.9187 - loss: 0.5373\n",
      "Test accuracy: 0.9090\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Load the DataFrame (assuming df is already loaded)\n",
    "# Assuming 'df' has your feature columns (0 to 5013) and the target column 'is_ai'\n",
    "\n",
    "# Split data into input features and target variable\n",
    "X = df_processed.drop(columns=['is_ai']).values  # All columns except 'is_ai'\n",
    "y = df_processed['is_ai'].values  # Target variable (whether it's AI or not)\n",
    "\n",
    "# Step 2: Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Reshape input data for CNN (this is important for CNN's expected input)\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))  # Reshaping for CNN (samples, features, channels)\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))  # Same for the test data\n",
    "\n",
    "# Step 4: Define the CNN Model\n",
    "model = models.Sequential([\n",
    "    layers.Conv1D(64, 3, activation='relu', input_shape=(X_train.shape[1], 1)),  # Conv1D layer for text features\n",
    "    layers.MaxPooling1D(2),  # Max pooling layer\n",
    "    layers.Conv1D(128, 3, activation='relu'),  # Another Conv1D layer for more feature extraction\n",
    "    layers.MaxPooling1D(2),\n",
    "    layers.Flatten(),  # Flatten the 1D features into a vector\n",
    "    layers.Dense(128, activation='relu'),  # Fully connected layer\n",
    "    layers.Dense(1, activation='sigmoid')  # Output layer (sigmoid for binary classification)\n",
    "])\n",
    "\n",
    "# Step 5: Compile the Model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Step 6: Train the Model\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Step 7: Evaluate the Model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('cnn_model_1.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Voting Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier Accuracy: 0.9200\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.96      0.92       987\n",
      "           1       0.96      0.88      0.92      1013\n",
      "\n",
      "    accuracy                           0.92      2000\n",
      "   macro avg       0.92      0.92      0.92      2000\n",
      "weighted avg       0.92      0.92      0.92      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import joblib\n",
    "\n",
    "# Load the pre-trained Random Forest and SVM models\n",
    "rf_model = joblib.load('rf_model_1.pkl')     # Random Forest model\n",
    "svm_model = joblib.load('svm_model_1.pkl')   # SVM model\n",
    "\n",
    "# Prepare your data (X, y). Assuming X and y are already preprocessed\n",
    "# Example of splitting the data if not done already:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the Voting Classifier with only Random Forest and SVM models\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', rf_model),     # Random Forest model\n",
    "        ('svm', svm_model)    # SVM model\n",
    "    ],\n",
    "    voting='hard'  # Use 'hard' for majority class voting (you can also use 'soft' for probability-based voting)\n",
    ")\n",
    "\n",
    "# Fit the Voting Classifier on the training data\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions using the Voting Classifier\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model using accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Voting Classifier Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Print detailed classification report for precision, recall, and f1-score\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
